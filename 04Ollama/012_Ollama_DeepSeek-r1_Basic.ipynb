{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q langchain\n",
    "# %pip install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬 Ollama로 설치한 deepseek-r1:1.5b 모델을 사용하기\n",
    "##### ollama run deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosta\\AppData\\Local\\Temp\\ipykernel_660\\633809197.py:6: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"deepseek-r1:1.5b\")\n",
      "C:\\Users\\kosta\\AppData\\Local\\Temp\\ipykernel_660\\633809197.py:15: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'question': 'What is LangChain?', 'text': '<think>\\n\\n</think>\\n\\nLangChain is a Python library designed to simplify the process of creating and training large language models. It provides an intuitive interface for defining the structure of your model, such as input fields, outputs, and constraints, allowing you to guide the learning process without needing deep expertise in NLP or machine learning.\\n\\n### Key Features of LangChain:\\n1. **Intuitive AI Design**: Simplifies the design of large language models by providing a user-friendly interface.\\n2. **Constraint-Based Learning**: Allows you to specify rules (e.g., \"long sentences are better\") that guide the model\\'s training.\\n3. **Extensive Training Data**: Can work with pre-collected datasets, including text from books, articles, Wikipedia, and more.\\n4. **Model Types**: Supports various language models like BERT, RoBERTa, and others through different interfaces (e.g., HuggingFace, SentencePiece).\\n5. **Integration with Other Tools**: Facilitates integration with tools for text generation, summarization, and deployment.\\n\\n### How to Use LangChain:\\n1. **Install the Package**: Use pip to install LangChain.\\n   ```bash\\n   pip install langchain\\n   ```\\n\\n2. **Define Your Model**:\\n   ```python\\n   from langchain.llms import BERTLlama\\n   llm = BERTLlama(\\n       \"https://raw.githubusercontent.com/huggingface/natural language-processing//master/annotation/bert-base-uncased.json\",\\n       n_ctx=2048,\\n       n_threads=1,\\n       n_jobs=-1\\n   )\\n   ```\\n\\n3. **Prepare Data**:\\n   ```python\\n   from langchain.data import get_text_fromURL\\n   text = get_text_fromURL(\"https://example.com\")\\n   ```\\n\\n4. **Train the Model**:\\n   ```python\\n   model, history = llm(text)\\n   ```\\n\\n5. **Use the Model**:\\n   ```python\\n   response = llm.getAnswer(\\n       q=\"What is LangChain?\",\\n       inputs=[\"what is langchain\"],\\n       outputs=[\"answer\"]\\n   )\\n   print(response[\"choices\"][\"answer\"])\\n   ```\\n\\nLangChain streamlines the process of creating and training models, making it accessible to a broader audience while still providing powerful capabilities for specific use cases.'}\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "LangChain is a Python library designed to simplify the process of creating and training large language models. It provides an intuitive interface for defining the structure of your model, such as input fields, outputs, and constraints, allowing you to guide the learning process without needing deep expertise in NLP or machine learning.\n",
      "\n",
      "### Key Features of LangChain:\n",
      "1. **Intuitive AI Design**: Simplifies the design of large language models by providing a user-friendly interface.\n",
      "2. **Constraint-Based Learning**: Allows you to specify rules (e.g., \"long sentences are better\") that guide the model's training.\n",
      "3. **Extensive Training Data**: Can work with pre-collected datasets, including text from books, articles, Wikipedia, and more.\n",
      "4. **Model Types**: Supports various language models like BERT, RoBERTa, and others through different interfaces (e.g., HuggingFace, SentencePiece).\n",
      "5. **Integration with Other Tools**: Facilitates integration with tools for text generation, summarization, and deployment.\n",
      "\n",
      "### How to Use LangChain:\n",
      "1. **Install the Package**: Use pip to install LangChain.\n",
      "   ```bash\n",
      "   pip install langchain\n",
      "   ```\n",
      "\n",
      "2. **Define Your Model**:\n",
      "   ```python\n",
      "   from langchain.llms import BERTLlama\n",
      "   llm = BERTLlama(\n",
      "       \"https://raw.githubusercontent.com/huggingface/natural language-processing//master/annotation/bert-base-uncased.json\",\n",
      "       n_ctx=2048,\n",
      "       n_threads=1,\n",
      "       n_jobs=-1\n",
      "   )\n",
      "   ```\n",
      "\n",
      "3. **Prepare Data**:\n",
      "   ```python\n",
      "   from langchain.data import get_text_fromURL\n",
      "   text = get_text_fromURL(\"https://example.com\")\n",
      "   ```\n",
      "\n",
      "4. **Train the Model**:\n",
      "   ```python\n",
      "   model, history = llm(text)\n",
      "   ```\n",
      "\n",
      "5. **Use the Model**:\n",
      "   ```python\n",
      "   response = llm.getAnswer(\n",
      "       q=\"What is LangChain?\",\n",
      "       inputs=[\"what is langchain\"],\n",
      "       outputs=[\"answer\"]\n",
      "   )\n",
      "   print(response[\"choices\"][\"answer\"])\n",
      "   ```\n",
      "\n",
      "LangChain streamlines the process of creating and training models, making it accessible to a broader audience while still providing powerful capabilities for specific use cases.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 deepseek-r1 모델을 로드\n",
    "llm = Ollama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Q: {question}\\nA:\"\n",
    ")\n",
    "\n",
    "# LLMChain 생성\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# 질문을 입력하고 모델의 응답을 받음\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "# 결과 출력\n",
    "print(type(response))\n",
    "print(response)\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 최신버전 LangChain에서는 ChatOllama와 RunnableSequence(prompt | llm) 를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "# prompt_template = PromptTemplate.from_template(\"Q: {question}\\nA:\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"What is Pyhon?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"파이썬은 무엇인가요?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-app-qc8Qb1Xk-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
