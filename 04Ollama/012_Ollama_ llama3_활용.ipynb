{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q langchain\n",
    "%pip install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬 Ollama로 설치한 gemma2 모델을 사용하기\n",
    "##### ollama run gemma2\n",
    "\n",
    "- `ChatOllama` 를 활용한 LLM 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\"LangChain\"는 언어.chain이라는 라이브러리의 약자입니다. 언어.chain은 다양한 인공지능 모델을 연결하여 특정 작업을 수행할 수 있는 패키지를 제공하는 Python 패키지입니다.\\n\\n이 패키지는 다음과 같은 주요 특징과 기능을 가지고 있습니다:\\n\\n1. 여러 모델을 쉽게 연결: 여러 대형 모델(Ex: BERT, T5 등)을 간편하게 연결하여 다양한 작업을 수행할 수 있게 합니다.\\n2. API 호출: 모델 인식 및 사용을 간소화하여 코드 중복을 최소화합니다.\\n3. 빠른 템플릿 생성: 특정 작업(Ex: 챗봇, 약물 추천 등)에 대한 빠르고 효율적인 템플릿을 제공합니다.\\n\\n따라서, 언어.chain 패키지는 다양한 모델을 연결하여 핵심 기능의 코드 작성 중복을 줄이고, 훈련 과정에서 발생하는 비용을 절감하는데 도움이 되는 도구입니다.' additional_kwargs={} response_metadata={'model': 'qwen2.5:1.5b', 'created_at': '2025-07-01T05:01:49.6063749Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14965056800, 'load_duration': 19700700, 'prompt_eval_count': 37, 'prompt_eval_duration': 66011200, 'eval_count': 247, 'eval_duration': 14877749700, 'model_name': 'qwen2.5:1.5b'} id='run--6ba4431c-24ad-485e-9872-9ce7e125e4cc-0' usage_metadata={'input_tokens': 37, 'output_tokens': 247, 'total_tokens': 284}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "response = llm.invoke(\"LangChain은 무엇인가요?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬 Ollama로 설치한 llama3.2 모델을 사용하기\n",
    "##### ollama run llama3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '\\nkl grade 3이고 , 골극의 개수는 3개, mJSW는 정상 대비 25% 감소율을 보이고 있어, OA가 48개월 내에 진행될 가능성이 60%일 때 종합적진 진단을 해줘. 단, 한국어로 답변해줘\\n', 'text': '이 질문과 관련된 정확한 진단 결과를 제공하기 위해서는 복잡한 질병 및 체계적인 분석이 필요합니다. 하지만 일반적으로 이런 상황에서 종합적 진단을 해보자면:\\n\\n1. **골근부손절**의 개수 3개는 보통 염증, 통증 등으로 인해 발생할 수 있습니다.\\n2. **MJSW(제반 장질환성 유전병)**의 감소율이 25%라니, 이것은 뇌 손실과 관련이 있을 가능성이 높아 보입니다.\\n3. **OA(Osteoarthritis - 골관절염)**가 48개월 내에 진행될 가능성 60%라고 하면, 이것은 흔히 말하는 \"골간결\"이나 \"골류량\"이라는 병이 예상됩니다.\\n\\n이러한 상황에서 종합적으로 진단을 해보자면:\\n\\n1. **골근부손절**의 개수는 염증 또는 통증으로 인해 발생할 가능성이 크다.\\n2. **MJSW**는 25% 감소율이기 때문에 뇌 손실에 대한 약간의 의심이 있을 수 있습니다.\\n3. **OA(Osteoarthritis - 골관절염)**가 예상되는 경우가 주목해야 할 점으로 볼 수 있으며, 이는 주로 48개월 내에 진행될 가능성이 높은 것으로 보입니다.\\n\\n만약 한정된 정보와 예측을 바탕으로 진단을 하려면:\\n\\n- 골근부손절의 개수: 염증이나 통증으로 인해 발생할 수 있음\\n- MJSW의 감소율: 의심스러운 뇌 손실 가능성이 있음\\n- OA(Osteoarthritis - 곩관절염): 예상되는 경우, 주로 48개월 내에 진행될 가능성 있어 보임\\n\\n이런 종류의 진단을 해두는 것은 복잡한 질병진단과 관련된 것으로 보이고, 실제 진료에서 발생하는 상황은 항상 복잡하며 개인적으로 맞춤형으로 이해해야 할 것입니다.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델을 로드\n",
    "# llm = Ollama(model=\"qwen2.5:1.5b\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Q: {question}\\nA:\"\n",
    ")\n",
    "\n",
    "# LLMChain 생성\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# 질문을 입력하고 모델의 응답을 받음\n",
    "#question = \"What is LangChain?\"\n",
    "question = \"\"\"\n",
    "kl grade 3이고 , 골극의 개수는 3개, mJSW는 정상 대비 25% 감소율을 보이고 있어, OA가 48개월 내에 진행될 가능성이 60%일 때 종합적진 진단을 해줘. 단, 한국어로 답변해줘\n",
    "\"\"\"\n",
    "#question = \"France의 수도는 어디입니까?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "# 결과 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 질문과 관련된 정확한 진단 결과를 제공하기 위해서는 복잡한 질병 및 체계적인 분석이 필요합니다. 하지만 일반적으로 이런 상황에서 종합적 진단을 해보자면:\n",
      "\n",
      "1. **골근부손절**의 개수 3개는 보통 염증, 통증 등으로 인해 발생할 수 있습니다.\n",
      "2. **MJSW(제반 장질환성 유전병)**의 감소율이 25%라니, 이것은 뇌 손실과 관련이 있을 가능성이 높아 보입니다.\n",
      "3. **OA(Osteoarthritis - 골관절염)**가 48개월 내에 진행될 가능성 60%라고 하면, 이것은 흔히 말하는 \"골간결\"이나 \"골류량\"이라는 병이 예상됩니다.\n",
      "\n",
      "이러한 상황에서 종합적으로 진단을 해보자면:\n",
      "\n",
      "1. **골근부손절**의 개수는 염증 또는 통증으로 인해 발생할 가능성이 크다.\n",
      "2. **MJSW**는 25% 감소율이기 때문에 뇌 손실에 대한 약간의 의심이 있을 수 있습니다.\n",
      "3. **OA(Osteoarthritis - 골관절염)**가 예상되는 경우가 주목해야 할 점으로 볼 수 있으며, 이는 주로 48개월 내에 진행될 가능성이 높은 것으로 보입니다.\n",
      "\n",
      "만약 한정된 정보와 예측을 바탕으로 진단을 하려면:\n",
      "\n",
      "- 골근부손절의 개수: 염증이나 통증으로 인해 발생할 수 있음\n",
      "- MJSW의 감소율: 의심스러운 뇌 손실 가능성이 있음\n",
      "- OA(Osteoarthritis - 곩관절염): 예상되는 경우, 주로 48개월 내에 진행될 가능성 있어 보임\n",
      "\n",
      "이런 종류의 진단을 해두는 것은 복잡한 질병진단과 관련된 것으로 보이고, 실제 진료에서 발생하는 상황은 항상 복잡하며 개인적으로 맞춤형으로 이해해야 할 것입니다.\n"
     ]
    }
   ],
   "source": [
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 최신버전 LangChain에서는 ChatOllama와 RunnableSequence(prompt | llm) 를 사용해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "#from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "#llm = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "# prompt_template = PromptTemplate.from_template(\"Q: {question}\\nA:\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-app-qc8Qb1Xk-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
